# -*- coding: utf-8 -*-
"""time_series.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1soCrvGqrX0vG9jAUdyrEfCK9wRjzryKG

<h2><center>Прогнозирование временных рядов</center></h2>
"""

!pip install -r requirements.txt

import warnings
import gdown
import pandas as pd
import numpy as np
from copy import deepcopy
from etna.analysis import acf_plot
from etna.analysis import cross_corr_plot
from etna.analysis import distribution_plot
from etna.analysis import plot_correlation_matrix
from etna.transforms import LagTransform
from etna.transforms import LinearTrendTransform
warnings.filterwarnings("ignore")

"""В данной задаче мы будем решать одну из актуальных практических задач, с которыми, в том числе, сталкиваются разработчики ETNA - прогнозирование объема необходимой наличности в банкоматах. Пожалуй, ни для кого не секрет, что отсутствие необходимой суммы в банкомате не делает клиента банка счастливее. В то же время, избыток заправленной в банкомат наличности приводит к упущенной выгоде - ведь, в конечном счете, эти деньги могли быть размещены в качестве краткосрочного депозита на межбанковском рынке. Для кредитной организации с обширной сетью банкоматов даже незначительное улучшение качества предсказания способно внести значительный вклад в прибыльность этой ветки бизнеса.

В качестве инструмента воспользуемся библиотекой ETNA, документацию можно прочитать [тут](https://docs.etna.ai/stable/), и [чат](https://t.me/etna_support) комьюнити.

Мы будем использовать дневные данные реальных банкоматов для чего возьмем мультисегментный датасет.
"""

url = 'https://gist.githubusercontent.com/Polzovat123/4d6d5e544e93429c2d3db29196e9c918/raw/24b27e60f587128d71678e267f51cd48472c6a84/atms_daily_cash.csv'
output = 'atms_daily_cash.csv'
gdown.download(url=url, output=output, quiet=False, fuzzy=True)

df = pd.read_csv('atms_daily_cash.csv', index_col=False)

"""Полученные данные были просуммированы по банкоматам и дням эксплуатации, после чего залиты в gist, откуда мы и берем их.

Взглянем на то, что представляют из себя данные после этих действий:
"""

df.head()

"""### EDA

"""

from etna.datasets import TSDataset

# чтобы говорить на одном языке с этной переименуем столбцы
df.rename(columns={'datetime': 'timestamp', 'operation_value': 'target', 'atm_id': 'segment'}, inplace=True)
ts = TSDataset(df, freq="D")
ts1 = TSDataset(df, freq="D")
ts2 = TSDataset(df, freq="D")
ts3 = TSDataset(df, freq="D")
ts.head(5)

"""Сначала глянем на простые графички, поищем там каких-нибудь закономерностей"""

ts.plot()

"""Так, конечно, мало что можно понять. Можно отметить, что на нескольких графиках наблюдалось много денег в районе сентября и января. Возможно, это связанно с праздниками в эти периоды - родственники кладут денежку чтобы перевести ее младшим членам семьи на день знаний, родственники кладут деньжата чтобы передарить их потом родным из других городов. Для нормального анализа нужно смотреть дальше

Давайте теперь посмотрим на тренды, это куда интереснее должно быть (рассматриваются полиномы 1, 2, 3 степеней)
"""

from etna.analysis import plot_trend
trends = [
    LinearTrendTransform(in_column="target", poly_degree=1),
    LinearTrendTransform(in_column="target", poly_degree=2),
    LinearTrendTransform(in_column="target", poly_degree=3),
]
plot_trend(ts, trend_transform=trends)

"""Глазками замечаем что тренд у банкоматов  восходящий (где-то сильнее, где-то слабее) и только у 102 банкомата начался нисходящий тренд в полиномах 2й и 3й степени. Это может говорить о том, что люди со временем все больше переходят на безналичные способы оплаты

Хотим еще посмотреть на сезонность - давайте посмотрим
"""

from etna.analysis import seasonal_plot

seasonal_plot(ts=ts, cycle="quarter")

seasonal_plot(ts=ts, cycle="month")

"""Сезонность не обнаружена

Попробуем посмотреть на корреляции между временными рядами
"""

lags = LagTransform(in_column="target", lags=[1, 7], out_column="lag")
ts1.fit_transform([lags])

plot_correlation_matrix(ts1, segments=["100", "74", "97"], method="spearman", vmin=0.5, vmax=1)

"""Видим супер низкую корреляцию между тремя случайно-выбранными банкоматами, можно делать вывод об отсутствии корреляции между временными рядами разных банкоматов

Чтобы не быть голословными построим еще кросс-корреляцию
"""

cross_corr_plot(ts, maxlags=100)

"""Чтобы утверждать о взаимосвязи рядов, значения кросс-корреляционной функции должно стремится к единице. У нас же только у нескольких пар значение превысило 0.6, значит отвергаем гипотезу о наличии корреляции между временными рядами

Посмотрим еще на автокорреляциию
"""

acf_plot(ts, lags=30)

"""Замечен для нескольких временных рядов значимый седьмой лаг - это позволяет говорить о недельной сезонности (которую было бы сложно отследить через графики сезонности, хорошо что решили использовать этот инструмент). Вообще, недельная сезонность звучит здраво, исходя из житейского опыта

### 2. Работа с пропущенными значениями и выбросами

определяем, есть ли пропуски
"""

ts.describe()

"""заполняем пропуски скользящим средним"""

from etna.transforms import TimeSeriesImputerTransform
from etna.analysis import plot_imputation
imputer = TimeSeriesImputerTransform(strategy="running_mean", window=30)
plot_imputation(imputer=imputer, ts=ts2)

"""в ts2 сохраняем копию с заполненными пропусками"""

ts2.fit_transform([imputer])

"""Проверим ряды на наличие выбросов

Начнем с медианного метода. За окно берем 30 дней (месяц)
"""

from etna.analysis import get_anomalies_density
from etna.analysis import get_anomalies_median
from etna.analysis import plot_anomalies
from etna.analysis import plot_anomalies_interactive

anomalies = get_anomalies_median(ts, window_size=30)
plot_anomalies(ts, anomalies)

anomalies = get_anomalies_density(ts, window_size=30, distance_coef=1, n_neighbors=4)
plot_anomalies(ts, anomalies)

"""Визуально метод с плотностью лучше находит выбросы, но поиграемся с интерактивным графиком и различными наборами параметров для определенного сегмента (взяла 105й, потому что кажется, это один из немногих, где я глазками могу определить, что является выбросом, а что - закономерное поведение ряда)"""

segment = "105"
method = get_anomalies_density
params_bounds = {"window_size": (10, 300, 10), "distance_coef": (0.1, 5, 0.1), 'n_neighbors' : (1, 10, 1),}
plot_anomalies_interactive(ts=ts, segment=segment, method=method, params_bounds=params_bounds)

segment = "105"
method = get_anomalies_median
params_bounds = {"window_size": (10, 300, 10), "alpha": (0.1, 5, 0.1)}
plot_anomalies_interactive(ts=ts, segment=segment, method=method, params_bounds=params_bounds)

"""На мой взгляд анализ по плотности точнее находит выбросы, поэтому далее буду использовать его с подобранными выше параметрами"""

anomalies = get_anomalies_density(ts, window_size=200, distance_coef=0.7, n_neighbors=9)
plot_anomalies(ts, anomalies)

from etna.transforms import DensityOutliersTransform
outliers = DensityOutliersTransform(in_column="target", window_size=200, distance_coef=0.7, n_neighbors=9)
ts.fit_transform([outliers])
ts.plot()

"""### 3. Построение Prophet"""

from etna.pipeline import Pipeline
from etna.models import ProphetModel
from etna.metrics import SMAPE, MAE
from etna.analysis import plot_backtest

HORIZON = 5

ts3.fit_transform([outliers])
ts3.fit_transform([imputer])

model = ProphetModel()
pipeline = Pipeline(model=model, horizon=HORIZON)
metrics1, forecast1, fold_info1 = pipeline.backtest(ts=ts3, metrics=[SMAPE(), MAE()])

metrics1.head()

plot_backtest(forecast1, ts3, history_len=50)

from etna.analysis import plot_metric_per_segment
plot_metric_per_segment(metrics_df=metrics1, metric_name="SMAPE", ascending=True)

plot_metric_per_segment(metrics_df=metrics1, metric_name="MAE", ascending=True)

"""Визуальный анализ графиков и метрик (в которых кстати название осей перепутано) показывает, что прогноз получился очень хиленький. Пока сравнивать не с чем, но хочется верить, что есть способы поточнее

Для сравнения со следующими моделями еще средние значения выведу
"""

metrics1[['SMAPE', 'MAE']].mean()

"""### 4. Иерархический временной ряд

Вопрос, поставленный в прошлом задании, тем не менее, естественным образом подводит нас к концепции **иерархического временного ряда** (когда один ряд состоит из других в качестве компонент). Это полезная концепция, которая может встретиться во многих задачах.
"""

from etna.datasets import HierarchicalStructure
from etna.pipeline import HierarchicalPipeline
from etna.reconciliation import TopDownReconciliator

new_df = df.copy()
# отрицательные значения заменим на нолик, иначе все ломается
new_df['target'] = new_df['target'].apply(lambda x: max(x, 0))

segment_list = new_df['segment'].astype(str).unique().tolist()
hierarchical_structure = HierarchicalStructure(level_structure={"total": segment_list}, level_names=["total", "atm_id"])

h_ts = TSDataset(new_df, freq='D', hierarchical_structure=hierarchical_structure)
h_ts.head()

ahp_reconciliator = TopDownReconciliator(
    target_level="atm_id", source_level="total", method="AHP", period=7
)
ahp_reconciliator.fit(h_ts)

"""Примените найденные на предыдущих этапах преобразования, очистку от выбросов уже к иерархическому датасету и запустим на нем Prophet с MAE на кросс-валидации."""

pipeline_h = HierarchicalPipeline(
    transforms=[imputer,outliers],
    model=ProphetModel(),
    reconciliator=ahp_reconciliator,
    horizon=HORIZON
)

metrics_h, forecast_h, fold_info_h = pipeline_h.backtest(h_ts, metrics=[SMAPE(), MAE()], n_folds=10, aggregate_metrics=True)

metrics_h

metrics_h[['SMAPE', 'MAE']].mean()

"""Средние ошибки выросли, а значит качество прогноза стало хуже чем было в предыдущей части

### 5. Построение признаков

Вернемся к нашему исходному мультисегментному ряду - теперь поработаем с моделями, которые требуют построения признаков - `ARIMA` и `CatBoost`. Построим для них признаки, и попробуем при помощи них добиться улучшения качества.
"""

from etna.analysis import acf_plot, stl_plot
from etna.ensembles import DirectEnsemble, StackingEnsemble, VotingEnsemble
from etna.models import (CatBoostMultiSegmentModel, CatBoostPerSegmentModel,
                         AutoARIMAModel)
from etna.transforms import STLTransform, LagTransform, SegmentEncoderTransform

acf_plot(ts, lags=30)

"""Как и ранее, много где виден значимый 7й лаг"""

stl_plot(ts3, period = 7)

lag_transf = LagTransform(in_column="target", lags=[7], out_column="lag")
stl = STLTransform('target', 7)

"""Для ясности: ts - удалены выбросы, ts1 - использовался только в корреляции, ts2 - заполнены пропуски, ts3 - удалены выбросы а потом заполнены пропуски. Соответственно для заданий ниже использую ts3

Начимаем эксперименты с аримы
"""

pipeline = Pipeline(
    transforms=[lag_transf],
    model=AutoARIMAModel(),
    horizon=HORIZON
)

metrics_a, forecast_a, fold_info_a = pipeline.backtest(ts=ts3, metrics=[SMAPE(), MAE()], n_folds=5, n_jobs=2, aggregate_metrics=True)

metrics_a

metrics_a[['SMAPE', 'MAE']].mean()

"""Запускаем катбуст на мультисегмент"""

model = CatBoostMultiSegmentModel()
pipeline = Pipeline(model=model, transforms=[lag_transf, stl], horizon=HORIZON)

metrics_cm, forecast_cm, fold_info_cm = pipeline.backtest(ts = ts3, metrics=[SMAPE(), MAE()], n_folds=5, n_jobs=2, aggregate_metrics=True)
metrics_cm

metrics_cm[['SMAPE', 'MAE']].mean()

"""А теперь катбуст на каждый сегмент"""

model = CatBoostPerSegmentModel()
pipeline = Pipeline(model=model, transforms=[lag_transf, stl], horizon=HORIZON)

metrics_ps, forecast_ps, fold_info_ps = pipeline.backtest(ts=ts3, metrics=[SMAPE(), MAE()], n_folds=5, n_jobs=2, aggregate_metrics=True)
metrics_ps

metrics_ps[['SMAPE', 'MAE']].mean()

"""Все модели этой части сработали в среднем хуже чем профет. Арима оказалась лучшей из 3х, а также обошла иерархический ряд, а вот катбуст подвел :("""